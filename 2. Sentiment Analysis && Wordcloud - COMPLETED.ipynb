{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6464de5d-267f-4b45-b10d-c1ca7086e5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Essential Packages\n",
    "# pip install transformers scikit-learn matplotlib seaborn wordcloud tqdm\n",
    "# pip install torch torchaudio torchvision\n",
    "# pip install charset-normalizer\n",
    "# !pip install --upgrade jupyter ipywidgets\n",
    "# !jupyter lab build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b5e9190-5fb6-45a6-a21f-9c52f62f4f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajesh\\anaconda3\\envs\\gpu\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fe65bed-c3b8-4343-8012-63851b4f63df",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "batch_size=32\n",
    "rating_positive_threshold=4.0\n",
    "sentiment_pos_threshold=0.6\n",
    "weight_rating=0.6\n",
    "weight_text=0.4\n",
    "alpha_composite=0.7\n",
    "lda_n_topics=4\n",
    "lda_max_features=2000\n",
    "min_reviews_lda=10\n",
    "random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a2621bf-a9c3-4b1b-87e9-02d697511656",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir=Path(\".\")\n",
    "out_dir=Path(\"Output\")\n",
    "out_dir.mkdir(exist_ok=True,parents=True)\n",
    "csv_files=[\n",
    "    \"ACER_REVIEWS_FIXED.csv\",\n",
    "    \"ASUS_REVIEWS_FIXED.csv\",\n",
    "    \"DELL_REVIEWS_FIXED.csv\",\n",
    "    \"LENOVO_REVIEWS_FIXED.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4e758c0-14b2-4c95-a235-d05c0ece6b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_csv(input_dir: Path):\n",
    "    found_files=[]\n",
    "    for i in csv_files:\n",
    "        file_path=input_dir/i\n",
    "        if file_path.exists():\n",
    "            found_files.append(file_path)\n",
    "    return found_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0b4caa1-18ee-4bce-866d-c50f3393da91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_company_name(name: str):\n",
    "    s=name.lower()\n",
    "    if \"acer\" in s:\n",
    "        return \"acer\"\n",
    "    if \"asus\" in s:\n",
    "        return \"asus\"\n",
    "    if \"dell\" in s:\n",
    "        return \"dell\"\n",
    "    if \"lenovo\" in s:\n",
    "        return \"lenovo\"\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e1b426b-4f51-403c-aeee-ff77eecf1726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_concat(files):\n",
    "    dfs=[]\n",
    "    for f in files:\n",
    "        try:\n",
    "            df=pd.read_csv(f,encoding=\"utf-8\", low_memory=False)\n",
    "        except Exception:\n",
    "            df=pd.read_csv(f,encoding=\"latin1\", low_memory=False)\n",
    "        df[\"__source_file\"] = f.name\n",
    "        if \"company\" not in df.columns:\n",
    "            df[\"company\"]=infer_company_name(f.name)\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs,ignore_index=True,sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdc13042-8b5b-4d26-9569-7c616c24e3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autodetect(df: pd.DataFrame):\n",
    "    text_col=None\n",
    "    rating_col=None\n",
    "    lower={c.lower(): c for c in df.columns}\n",
    "    for c in [\"text\"]:\n",
    "        if c in lower:\n",
    "            text_col=lower[c];\n",
    "            break\n",
    "    for c in [\"rating\"]:\n",
    "        if c in lower:\n",
    "            rating_col=lower[c];\n",
    "            break\n",
    "    return text_col, rating_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dd43be9-0436-4402-b230-c18945358246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s):\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    s=str(s).lower()\n",
    "    s = re.sub(r\"http\\S+\", \" \", s)\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "119eae66-2d67-4e2a-94b1-acf7251c6e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wilson_lower_bound(pos, n, z=1.96):\n",
    "    if n==0:\n",
    "        return 0.0\n",
    "    phat=pos/n\n",
    "    denom= 1 + z*z/n\n",
    "    num= phat + z*z/(2*n) - z * math.sqrt((phat*(1-phat) + z*z/(4*n))/n)\n",
    "    return num/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f3d0afa-c6d2-461b-86f7-d0cd3f86a4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer= AutoTokenizer.from_pretrained(transformer_model)\n",
    "tf_model=TFAutoModelForSequenceClassification.from_pretrained(transformer_model, from_pt=True)\n",
    "label_map=tf_model.config.id2label if hasattr(tf_model.config, \"id2label\") else {0: \"NEGATIVE\", 1: \"POSITIVE\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0b51e88-2ff6-4fc3-9657-9bee90146cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(texts, batch_size=batch_size):\n",
    "    labels=[]\n",
    "    scores=[]\n",
    "    signed=[]\n",
    "    for i in range(0,len(texts),batch_size):\n",
    "        batch=texts[i:i+batch_size]\n",
    "        enc=tokenizer(batch, return_tensors=\"tf\", padding=True, truncation=True, max_length=256)\n",
    "        outputs=tf_model(enc)\n",
    "        logits=outputs.logits.numpy()\n",
    "        exp = np.exp(logits - np.max(logits,axis=1,keepdims=True))\n",
    "        probs= exp/np.sum(exp,axis=1,keepdims=True)\n",
    "        pred_idx=probs.argmax(axis=1)\n",
    "        pred_score=probs[np.arange(len(pred_idx)),pred_idx]\n",
    "        for idx,sc in zip(pred_idx, pred_score):\n",
    "            lbl=label_map[int(idx)] if int(idx) in label_map else str(idx)\n",
    "            labels.append(lbl)\n",
    "            scores.append(float(sc))\n",
    "            signed.append(float(sc) if str(lbl).lower().startswith(\"pos\") else -float(sc))\n",
    "    return labels,scores,signed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a3860a6-42f7-4abb-b288-d38fd39af826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    files=find_csv(input_dir)\n",
    "    df=load_concat(files)\n",
    "    text_col, rating_col= autodetect(df)\n",
    "    if \"company\" not in df.columns:\n",
    "        df[\"company\"]=df[\"__source_file\"].apply(infer_company_name)\n",
    "    df[\"company\"]=df[\"company\"].astype(str).str.lower().fillna(\"\")\n",
    "    mask = df[\"company\"] == \"\"\n",
    "    df.loc[mask, \"company\"] = df.loc[mask, \"__source_file\"].apply(infer_company_name)\n",
    "    df[\"review_text\"]=df[text_col].fillna(\"\").astype(str) if text_col else \"\"\n",
    "    df[\"clean_text\"]=df[\"review_text\"].apply(clean_text)\n",
    "    if rating_col:\n",
    "        df[\"rating_1_5\"] = pd.to_numeric(df[rating_col],errors='coerce')\n",
    "    else:\n",
    "        df[\"rating_1_5\"]=np.nan\n",
    "    df[\"rating_01\"]=df[\"rating_1_5\"].apply(lambda x: (x-1)/4 if not pd.isna(x) else np.nan)\n",
    "    texts=df[\"clean_text\"].fillna(\"\").astype(str).tolist()\n",
    "    labels,scores,signed=sentiment_analysis(texts,batch_size=batch_size)\n",
    "    df[\"transformer_label\"]=labels\n",
    "    df[\"transformer_score\"]=scores\n",
    "    df[\"transformer_signed\"]=signed\n",
    "    df[\"transformer_01\"]= (df[\"transformer_signed\"] + 1)/2.0\n",
    "    def combine_satisfaction(r01,t01):\n",
    "        try:\n",
    "            r01 = float(r01) if not pd.isna(r01) else np.nan\n",
    "        except (ValueError, TypeError):\n",
    "            r01 = np.nan  \n",
    "        try:\n",
    "            t01 = float(t01) if not pd.isna(t01) else np.nan\n",
    "        except (ValueError, TypeError):\n",
    "            t01 = np.nan\n",
    "        if not pd.isna(r01) and not pd.isna(t01):\n",
    "            return weight_rating * r01 + weight_text * t01\n",
    "        if not pd.isna(r01):\n",
    "            return r01\n",
    "        if not pd.isna(t01):\n",
    "            return t01\n",
    "        return np.nan\n",
    "    df[\"satisfaction\"]=df.apply(lambda r:combine_satisfaction([\"rating_01\"], r[\"transformer_01\"]), axis=1)\n",
    "    df[\"is_positive_rating\"]=df[\"rating_1_5\"].apply(lambda x: 1 if (not pd.isna(x) and float(x) >= rating_positive_threshold) else 0)\n",
    "    df[\"is_positive_text\"]=df[\"transformer_01\"].apply(lambda x: 1 if (not pd.isna(x) and float(x) >= sentiment_pos_threshold) else 0)\n",
    "    df[\"is_positive_review\"]= ((df[\"is_positive_rating\"] == 1) | (df[\"is_positive_text\"] == 1)).astype(int)\n",
    "\n",
    "    agg=df.groupby(\"company\").agg(\n",
    "        reviews_count=(\"review_text\",\"count\"),\n",
    "        positive_reviews=(\"is_positive_review\",\"sum\"),\n",
    "        avg_rating=(\"rating_1_5\",\"mean\"),\n",
    "        avg_transformer_signed=(\"transformer_signed\",\"mean\"),\n",
    "        avg_satisfaction=(\"satisfaction\",\"mean\")\n",
    "    ).reset_index()\n",
    "    scaler = MinMaxScaler()\n",
    "    agg[\"wilson_lower\"]=agg.apply(lambda r: wilson_lower_bound(int(r[\"positive_reviews\"]), int(r[\"reviews_count\"])),axis=1)\n",
    "    agg[\"reviews_count_scaled\"]=scaler.fit_transform(agg[[\"reviews_count\"]]).flatten()\n",
    "    agg[\"avg_satisfaction\"]=agg[\"avg_satisfaction\"].fillna(agg[\"avg_satisfaction\"].median())\n",
    "    agg[\"avg_satisfaction_scaled\"]=scaler.fit_transform(agg[[\"avg_satisfaction\"]]).flatten()\n",
    "    agg[\"composite_scaled\"] = alpha_composite * agg[\"avg_satisfaction_scaled\"] + (1 - alpha_composite) * agg[\"reviews_count_scaled\"]\n",
    "    agg[\"composite_wilson\"] = alpha_composite * agg[\"avg_satisfaction_scaled\"] + (1 - alpha_composite) * agg[\"wilson_lower\"]\n",
    "    out_dir.mkdir(exist_ok=True,parents=True)\n",
    "    agg.to_csv(out_dir / \"company_summary.csv\",index=False)\n",
    "    df.to_csv(out_dir / \"reviews_full.csv\", index =False)\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
